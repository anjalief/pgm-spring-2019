---
layout: distill
title: "Lecture 26: Gaussian processes (GPs) and elements of meta-learning"
description: GPs, kernel functions, (Deep) kernel learning and approximations, NPs, and meta-learning
date: 2019-04-22

lecturers:
  - name: Maruan Al-Shedivat
    url: "https://www.cs.cmu.edu/~mshediva/"

authors:
  - name: Anjalie Field  # author's full name
    url: "http://www.cs.cmu.edu/~anjalief/"  # optional URL to the author's homepage
  - name: Wenchao Du
    url: "#"
  - name: Sachin Kumar
    url: "#"

editors:
  - name: Paul Liang  # editor's full name
    url: "https://www.cs.cmu.edu/~pliang/"  # optional URL to the editor's homepage

abstract: >
  This lecture covers Gaussian processes, with extensions to kernel functions, deep kernel learning and approximations, Neural Processes as an approximation to Gaussian processes, and elements of meta-learning
---

## Learning Functions from Data

**Background**

Given a set of data points, we often want to learn a function that describes the data. One approach is to guess the parametric form of a function that could fit the data. Forms we might guess include:


<ul>
<li>Linear function of $\mathbf{w}$ and $\mathbb{x}$: $f(\mathbb{x}, \mathbf{w}) = \mathbf{w}^T\mathbb{x}$</li>

<li>Linear function of $\mathbf{w}$: $f(\mathbb{x}, \mathbf{w}) = \mathbf{w}^T\phi(\mathbb{x})$  where $\phi(\mathbb{x})$ is a vector basic function, i.e. $\phi(\mathbb{x}) = (1, \mathbb{x}, \mathbb{x}^2)$</li>

<li>Non-linear function of $\mathbf{w}$ and $\mathbb{x}$: $f(\mathbb{x}, \mathbf{w}) = \mathbb{g} (\mathbf{w}^T\phi(\mathbb{x}))$, i.e. Neural Network </li>
</ul>

We then choose an error measure and minimize with respect to $\mathbf{w}$: $E(\mathbf{w}) = \sum_{i=1}^n \left[f(\mathbb{x_i}, \mathbf{w}) - y(\mathbb{x_i}) \right]^2$


**Noise**

Additionally, we can explicitly account for noise in our model by introducting a noise function $\epsilon(x)$: $y(\mathbb{x}) = f(\mathbb(x), \mathbf{w}) + \epsilon(\mathbb{x})$

We commonly use i.d.d. additive Gaussian noise, where we take $\epsilon(x) = \mathcal{N}(0, \sigma^2)$. Then, we aim to maximize the likelihood of the data $p(\mathbf{y} \mid \mathbb{x}, \mathbf{w}, \sigma^2)$ with respect to $\sigma^2$, $\mathbf{w}$. The model and likelihood are given by:

<ul>
<li> Observation Model: $p(y(\mathbb{x}) | \mathbb{x}, \mathbf{w}, \sigma^2) = \mathcal{N}(y(\mathbb{x}); f(\mathbb{x}, \mathbf{w}), \sigma^2)$ </li>
<li> Likelihood: $p(\mathbf{y} | \mathbb{x}, \mathbf{w}, \sigma^2) = \prod_{i=1}^N \mathcal{N}(y(\mathbb{x_i}); f(\mathbb{x_i}, \mathbf{w}), \sigma^2)$ </li>
</ul>


**Regularlization**

This probabilistic approach helps us interpret the error measure in a deterministic way and gives a sense of the noise level $\sigma^2$. Thus, probabilistic methods provide an intuitive framework for representing uncertainty and model development. However, these approaches are prone to *overfitting* for flexible $f(\mathbb{x}, \mathbf{w})$. They achieve low error on the training data, but high error on test data.

One way to reduce overfitting is to use *regularization*. We can introdcuce a complexity penality to the log-likelihood or error function:

$$ \log p(\mathbf{y} | \mathbb{x}, \mathbf{w}) \propto -\frac{1}{2 \sigma^2} \sum_{i=1}^n(f(\mathbb{x_i}, \mathbf{w}) - y(\mathbb{x_i})^2) - \lambda \mathbb{w}^T\mathbb{w}  $$

However, this introduces new questions: how do we define complexity? and how much should we penalize complexity? In practice, we control the penalty by setting  $\lambda$ using cross-validation

**Bayesian Approach**

We can describe our data and models using Bayes' Rule:

$$posterior = \frac{likelihood * prior}{marginal\ likelihood}$$

$$ p(\mathbf{w} \mid \mathbf{y}, \mathbb{X}, \sigma^2) = \frac{p(\mathbf{y} \mid \mathbb{X}, \mathbf{w}, \sigma^2)p(\mathbf{w})}{p(\mathbf{y} \mid \mathbb{X}, \sigma^2)} $$

To make predictions over a test case, we can obtain a predictive distribution by marginalizing out $\mathbf{w}$:

$$p(y \mid x_{*}, \mathbf{y}, \mathbb{X}) = \int p(y \mid x_{*}, \mathbf{w}) p(\mathbf{w} \mid \mathbf{y}, \mathbb{X}) d \mathbf{w}$$

In this predictive distribution, we average over infinitely many models weighted by their posterior probabilities. There is no over-fitting, and complexity is automatically calibrated. This approach is useful because we are typically more interested in distributions over functions than in parameters $\mathbf{w}$.




## Introducing nonparametric models


**Comparison to parametric models**

In parametric models, we assume that all data can be represented using a fixed, finite number of parameters. (e.g. Mixture of K Gaussians, polynomial regression, neural networks). In nonparametric models, the number of parameters can grow with the sample size. The number of parameters may be random (e.g. kernel density estimation). In Bayesian nonparametrics, we allow for an infinite number of parameters a prior. Models of finite datasets will have only finite number of parameters; other parameters are integrated out. We can compare parametric Bayesian inference with nonparametric Bayesian inference:

*Parametric Bayesian Inference* | *Nonparametric Bayesian Inference*
$\mathcal{M}$ is represented as a finite set of parameters $\theta$ | $\mathcal{M}$ is a richer model (e.g. with an infinite set of parameters)
Parametric likelihood $x \sim p(\bullet \mid \theta)$ | Nonparametric likelihood $x \sim p(\bullet \mid \mathcal{M})$
Prior on $\theta$: $\pi(\theta)$ | Prior on $\mathcal{M}$: $\pi(\mathcal{M})$
Posterior distribution: $p(\theta \mid x) \propto p(x \mid \theta)\pi(\theta)$ | Posterior distribution: $p(\mathcal{M} \mid x) \propto p(x \mid \mathcal{M})\pi(\mathcal{M})$
Examples: Gaussian distribution prior + 2D Gaussian likelihood $\rightarrow$ Gaussian posterior distribution | Examples: Dirichlet Process Prior + Multinomial/Gaussian/Softmax Likelihood

**Weight space vs. Function space view**

Consider a simple linear model: $f(x) = a_0 + a_1x$ for $a_0, a_1 \sim \mathcal{N}(0, 1)$.

We can sample different weights ($a_0$, $a_1$) and graph the results (e.g. weight space view):

<figure id="weightspace-view" class="l-body-outset">
  <div class="row">
    <div class="col one">
      <img src="{{ 'assets/img/notes/lecture-26/weight_space.png' | relative_url }}" />
    </div>
  </div>
  <figcaption>
    <strong>Weight space view of a simple linear model</strong>
  </figcaption>
</figure>

However, we are more interested in the distribution over functions induced by the distribution over parameters, rather than the distribution over parameters. We can characterize the properties of these functions:

<d-math block>
\begin{aligned}
& f(x | a_0, a_1) = a_0 + a_1x \\
\\
& \mathbb{E}[f(x)] = \mathbb{E}[a_0] + \mathbb{E}[a_1]x = 0\\
\\
cov[f(x_b), f(x_c)] &= \mathbb{E}[f(x_b)f(x_c)] - \mathbb{E}[f(x_b)]\mathbb{E}[f(x_c)]\\

& = \mathbb{E}[a_0^2 + a_0a_1(x_b + x_c) + a_1^2x_bx_c] - 0\\

& = \mathbb{E}[a_0^2] + \mathbb{E}[a_0a_1(x_b + x_c)] + \mathbb{E}[a_1^2x_bx_c]\\

& = 1 + x_bx_c+ 0\\
& = 1 + x_bx_c
\end{aligned}
</d-math>

This gives the first and second moments of the function for random variables along the x-axis.

Using a little algebra, we can show that any collection of values from this set has a joint Gaussian distribution<d-cite key="rasmussen2003gaussian"></d-cite>:

<d-math block>
\begin{aligned}
\left[ f(x_1) ..f(x_N) \right] \sim \mathcal{N}(0, K)
\end{aligned}
</d-math>

where $K$ is defined by,

<d-math block>
\begin{aligned}
K_{ij} = cov(f(x_i), f(x_j)) = k(x_i, x_j) = 1 + x_bx_c
\end{aligned}
</d-math>

This is a Gaussian process

**Gaussian Process**

A Gaussian process (GP) is a collection of random variables, any finite number of which have a joint Gaussian distribution. We write $f(x) \sim \mathcal{GP}(m, k)$ to mean

<d-math block>
\begin{aligned}
\left[ f(x_1) ..f(x_N) \right] \sim \mathcal{N}(\mu, K) \\
\mu_i = m(x_i) \\
K_{ij} = k(x_i, x_j)
\end{aligned}
</d-math>

for any collection of input values $x_1...x_N$. Then, $f$ is a GP with mean function $m(x)$ and covariance kernel $k(x_i, x_j)$

As an example, consider Linear Basis Function Models:

* Model speficiations:

<d-math block>
\begin{aligned}
f(x, \mathbf{w}) = \mathbf{w}^T\phi(x)
\end{aligned}
</d-math>

<d-math block>
\begin{aligned}
p(\mathbf{w}) = \mathcal{N}(0, \Sigma_w)
\end{aligned}
</d-math>

* Moments of the induced distribution over functions:

<d-math block>
\begin{aligned}
& \mathbb{E}[f(x, \mathbf{w})] = m(x) = \mathbb{E}[\mathbf{w^T}]\phi(x) = 0\\
\\
cov[f(x_i), f(x_i)] &= k(x_i, x_j) = \mathbb{E}[f(x_i) f(x_j)] - \mathbb{E}[f(x_i)]\mathbb{E}[f(x_j)]\\

& = \phi(x_i)^T \mathbb{E}[\mathbf{w}\mathbf{w^T}]\phi(x_j) - 0\\

& = \phi(x_i)^T\Sigma_w\phi(x_j)
\end{aligned}
</d-math>


In this example, $f(x, \mathbf{w})$ is a Gaussian process, $f(x) \sim \mathcal{N}(m, k)$ with mean function $m(x) = 0$ and covariance kernel $k(x_i, x_j) = \phi(x_i)^T \Sigma_w\phi(x_j)$


We generally have more intuition about the functions that model our data than the weights $\mathbf{w}$ in a parametric model. We can express these intuitions using a covariance kernel. Additionally, the kernel controls the support and inductive biases of our model, and thus the model's ability to generalize to unseen data.
